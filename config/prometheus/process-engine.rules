ALERT HighConnectionPoolUsage
  IF (tomcat_datasource_numactive_total{context="/active-bpel"} / sa_engineresource_threadpoolmax) * 100 > 80
  FOR 1m
  LABELS { severity = "warning" }
  ANNOTATIONS {
    summary = "High connection pool usage on {{ $labels.instance }}",
    description = "{{ $labels.instance }} has a connection count above 80% for longer than 1 minute, (current value: {{ $value }}%)",
  }

ALERT HighProcessInMemoryCount
  IF (sa_systemmetrics_inmemoryprocessmetrics_processcount /sa_engineresource_processcount) * 100 > 75
  FOR 5m
  LABELS { severity = "warning" }
  ANNOTATIONS {
    summary = "High processes in memory count on {{ $labels.instance }}",
    description = "{{ $labels.instance }} is nearing max allowable processes in memory count, (current value: {{ $value }})",
  }
  
ALERT ProcessInMemoryCountExceeded
  IF sa_systemmetrics_inmemoryprocessmetrics_processcount > sa_engineresource_processcount
  FOR 10s
  LABELS { severity = "error" }
  ANNOTATIONS {
    summary = "Processes in memory count exceeded on {{ $labels.instance }}",
    description = "{{ $labels.instance }} has exceeded the max allowed processes in memory (current value: {{ $value }})",
  }
  
ALERT HighMessageRejectedRate
  IF rate(sa_serverstatistics_dispatchmsgrejected_overallcount[5m]) > .5
  FOR 30s
  LABELS { severity = "warning" }
  ANNOTATIONS {
    summary = "Rapidly rejecting messages on {{ $labels.instance }}",
    description = "{{ $labels.instance }} is rejecting requests at a rate of {{ $value }} messages per second.",
  }

ALERT HighHttpThreadCount
  IF (tomcat_threadpool_currentthreadsbusy{port="4430"} / 600) * 100 > 80
  FOR 1m
  LABELS { severity = "warning" }
  ANNOTATIONS {
    summary = "HTTP thread usage on {{ $labels.instance }} is nearing limit",
    description = "{{ $labels.instance }} has {{ $value }} active HTTP threads out of a total 600.",
  }
  
ALERT ClusterCommunicationsIssue 
  IF increase(sa_serverstatistics_clustercommunicationissue_overallcount[1m]) > 1
  FOR 5s
  LABELS { severity = "error" }
  ANNOTATIONS {
    summary = "Node {{ $labels.instance }} has detected a cluster communications issue.",
    description = "{{ $labels.instance }} has detected a cluster {{ $value }} communications issue(s), please check cluster health.",
  }
  
ALERT DatabaseConnectionAcquisitionTime
  IF sa_serverstatistics_databaseconnectionacquisitiontime_overallavg > 500
  FOR 5m
  LABELS { severity = "warning" }
  ANNOTATIONS {
    summary = "Node {{ $labels.instance }} database connection acquisition time too high",
    description = "{{ $labels.instance }} database connection acquisition time is higher that 500ms, (current value: {{ $value }}ms).",
  }
  
ALERT DeadlockRetryAttempts
  IF increase(sa_serverstatistics_deadlockretry_overallcount[1m]) > 1
  FOR 1s
  LABELS { severity = "warning" }
  ANNOTATIONS {
    summary = "Node {{ $labels.instance }} deadlock retry detected.",
    description = "{{ $labels.instance }} has had {{ $value }} deadlock retry attempts over the last minute.",
  }
  
ALERT EngineRemovedFromCluster 
  IF increase(sa_serverstatistics_engineremoved_overallcount[5m]) > 1
  FOR 1s
  LABELS { severity = "error" }
  ANNOTATIONS {
    summary = "Node {{ $labels.instance }} detected another engine has been removed from the cluster.",
    description = "{{ $labels.instance }} has detected another engine has been removed from the cluster.",
  }
  
ALERT FailedToLockProcess 
  IF increase(sa_serverstatistics_failedtolockprocess_overallcount[5m]) > 1
  FOR 1s
  LABELS { severity = "warning" }
  ANNOTATIONS {
    summary = "Node {{ $labels.instance }} failed to lock process.",
    description = "{{ $labels.instance }} failed to lock process {{ $value }} times over the last 5 minutes.",
  }

ALERT TimeToSaveProcess
  IF stddev_over_time(sa_serverstatistics_timetosaveprocess_overallavg{env="production"}[5m]) > 3
  FOR 5m
  LABELS { severity = "warning" }
  ANNOTATIONS {
    summary = "Node {{ $labels.instance }} increasing time to save process.",
    description = "{{ $labels.instance }} has had increasing time to save process over the last 5 minutes.",
  }
 
ALERT TimeToSaveProcessCritical
  IF stddev_over_time(sa_serverstatistics_timetosaveprocess_overallavg{env="production"}[5m]) > 6
  FOR 5m
  LABELS { severity = "error" }
  ANNOTATIONS {
    summary = "Node {{ $labels.instance }} increasing time to save process.",
    description = "{{ $labels.instance }} has had increasing time to save process over the last 5 minutes.",
  }

ALERT TimeToObtainProcess 
  IF stddev_over_time(sa_serverstatistics_timetoobtainprocess_overallavg{env="production"}[5m]) > 3
  FOR 5m
  LABELS { severity = "warning" }
  ANNOTATIONS {
    summary = "Node {{ $labels.instance }} increasing time to obtain process.",
    description = "{{ $labels.instance }} has had increasing time to obtain process over the last 5 minutes.",
  }
  
ALERT TimeToObtainProcessCritical
  IF stddev_over_time(sa_serverstatistics_timetoobtainprocess_overallavg{env="production"}[5m]) > 6
  FOR 5m
  LABELS { severity = "error" }
  ANNOTATIONS {
    summary = "Node {{ $labels.instance }} increasing time to obtain process.",
    description = "{{ $labels.instance }} has had increasing time to obtain process over the last 5 minutes.",
  }

ALERT CriticalStorageException
  IF increase(sa_serverstatistics_storageexceptions_overallcount[1m]) > 1
  FOR 1s
  LABELS { severity = "error" }
  ANNOTATIONS {
    summary = "Node {{ $labels.instance }} detected a critical storage exception.",
    description = "{{ $labels.instance }} detected a critical {{ $value }} storage exception(s).",
  }
  
ALERT WorkManagerStartDelay
  IF sa_serverstatistics_workmanagerstart_overallavg > 100
  FOR 5m
  LABELS { severity = "warning" }
  ANNOTATIONS {
    summary = "Node {{ $labels.instance }} work manager is experiencing delays.",
    description = "{{ $labels.instance }} time to start work in the work manager is {{ $value }}ms on average, about the 50ms threshold.",
  }

ALERT WorkManagerQueueExceedsThreads
  IF sa_systemmetrics_workmanagermetrics_queuedrequestcount > sa_systemmetrics_workmanagermetrics_activethreadcount
  FOR 1m
  LABELS { severity = "warning" }
  ANNOTATIONS {
    summary = "Node {{ $labels.instance }} work manager queue is greater than the number of threads executing.",
    description = "{{ $labels.instance }} work manager queue size is {{ $value }}, which is greater than the total number of threads executing.",
  }

 ALERT JmxExporterDown
  IF up == 0
  FOR 2m
  LABELS { severity = "error" }
  ANNOTATIONS {
    summary = "Node {{ $labels.instance }} jmx_exporter is not responding.",
    description = "{{ $labels.instance }} does not have a jmx_exporter which is responding normally please check jmx_exporter process or verify the ICRT node is available.",
  } 

ALERT HighCPUWarning
  IF java_operatingsystem_systemcpuload * 100 > 80
  FOR 5m
  LABELS { severity = "warning" }
  ANNOTATIONS {
    summary = "Node {{ $labels.instance }} CPU above 80%",
    description = "{{ $labels.instance }} has CPU usage above 80% for the past 15 minutes.",
  }
  
ALERT HighCPUError
  IF java_operatingsystem_systemcpuload * 100 > 95
  FOR 5m
  LABELS { severity = "error" }
  ANNOTATIONS {
    summary = "Node {{ $labels.instance }} CPU above 95%",
    description = "{{ $labels.instance }} has CPU usage above 95% for the past 5 minutes.",
  }
